\documentclass{homework}
\course{Math 5520H}
\author{Jim Fowler}

\begin{document}
\maketitle

\begin{inspiration}
It takes a very unusual mind to undertake the analysis of the obvious. \\
\byline{Alfred North Whitehead}
\end{inspiration}

\section{Terminology}

\begin{problem}
  What is a \textit{regular} singular point?  (cf.~\ref{defn-singular-point})
\end{problem}

\begin{problem}
  What is the \textit{indicial polynomial}?
\end{problem}

\section{Numericals}

\begin{problem}
  Use \textbf{the method of Frobenius} to find a solution to \(x^2 y'' - x y' + (1-x) \, y = 0\).

\end{problem}

\begin{solution}
  I think one can show that
  \[
    y(x) = x \int_0^\infty \cos \left(2 \sqrt{x} \sinh \theta\right) \, d\theta
  \]
  satisfies the equation.
\end{solution}

\begin{problem}\label{bessel-j0}The \textbf{Bessel equation} of order
  $\alpha$ is the equation
  \[
    x^2 \, y'' + x \, y' + (x^2 - \alpha^2) \, y = 0.
  \]
  Is the origin a regular singular point?  Verify that
  \[
    J_0(x) = \sum_{k=0}^\infty \frac{(-1)^k (x/2)^{2k}}{(k!)^2}
  \]
  satisfies the Bessel equation when $\alpha = 0$.
\end{problem}

\begin{problem}
  Check that $J'_0(x)$ satisfies \(x^2 \, y'' + x \, y' + (x^2 - 1) \, y = 0\).
  
  The convention is $J_1(x) = - J'_0(x)$ (which is akin to $\frac{d}{dx} \cos x = - \sin x$).
\end{problem}

\section{Exploration}

\begin{problem}
  Let's generalize the second-order equation we solved in
  \ref{second-order-euler-cauchy}.  Solve the
  $n^{\mbox{\scriptsize th}}$-order differential equation
  \[
    \sum_{k=0}^n a_k x^k y^{(k)} = 0
  \]
  with $a_n = 1$.

  \textit{Hint:} Make a change of variables.  Let $y(x) = z(e^x)$ and
  find a linear equation with constant coefficients satisfied by $z$.
\end{problem}

\begin{problem}
  Find a recurrence for $a_k$ so that $y = \sum a_k x^k$ satisfies \(x^2 y'' - y' - y = 0\).

  This differential equation has a singular point at $x = 0$.  Is it a
  \textit{regular} singular point?  What is the radius of convergence
  of your power series?
\end{problem}

\begin{solution}
  Also, I want to show off other phenomena.

  Consider $x^3 y'' + x y' - y = 0$.  Since $y = x$ is a solution, we
  can use variation of parameters to find another solution.
\end{solution}

\begin{problem}
  Consider the three equations
  \begin{align}
    4xy'' + 2y' + y &= 0 \label{eqn-one}\\
    x y'' + y' - y &= 0 \label{eqn-two}\\
    x y'' + y &= 0 \label{eqn-three}
  \end{align}
  and note that in each case $x=0$ is a regular singlar point, but the
  indicial polynomials are rather different.  For which of
  \eqref{eqn-one}, \eqref{eqn-two}, \eqref{eqn-three} are the roots of
  the indicial equation distinct?  When they are, do the distinct
  roots differ by an integer?

  How does the different behavior of the indicial polynomials manifest
  itself in your method for finding solutions?
\end{problem}


\begin{problem}\label{finding-zeroes-of-bessel}Check that $y(x) = J_0(x) \sqrt{x}$ satisfies, for $x > 0$,
  \[
    y''(x) + \left( 1 + \frac{1}{4x^2} \right) y(x) = 0.
  \]
  Suppose $y(x) > 0$ for all $x \in [2 n \pi, (2n+1) \pi]$ and consider
  \[
    \left( y \sin' - y' \sin \right)' = y \sin '' - y'' \sin. % = \frac{1}{4x^2} y > 0
  \]
  Then integrate over $[2n \pi, (2n+1) \pi]$ to get a
  contradiction\ldots to what?

  What can you conclude about the zeroes of $J_0(x)$?
\end{problem}

\begin{solution}
  There must be a zero of $y(x)$ in the interval $x \in [2 n \pi, (2n+1) \pi]$, and therefore there are infinitely many zeroes of $J_0(x)$.
\end{solution}

\begin{problem}The technique in \ref{finding-zeroes-of-bessel} can also be applied to Airy's equation (\ref{introduction-airy-function}).  Suppose $f$ satisfies
  \[
    f''(x) - x f(x) = 0
  \]
  and show that there are infinitely many $x < 0$ such that
  $f(x) = 0$.

  Can you relate this to the Sturm separation theorem
  (\ref{sturm-separation})?
\end{problem}

\begin{problem}
  Show that, when $x > 0$ is such that $J_0(x) = 0$, then
  $J'_0(x) \neq 0$, meaning that the zeroes are \textbf{simple}
  zeroes.  (That the zeroes of $J_0(x)$ and $J_1(x)$ only coincide when
  $x=0$ is the first case of \textbf{Bourget's hypothesis.})

  \textit{Hint:} What would happen if both $J_0(x)$ and $J'_0(x)$
  vanishes?
\end{problem}

\section{Prove or Disprove and Salvage if Possible}

\begin{problem}
  The $n^{\mbox{\scriptsize th}}$ \textbf{Laguerre polynomial}, defined as
  \(L_n(x) = e^x \displaystyle\frac{d^n}{dx^n} (x^n e^{-x})\),
  satisfies the Laguerre equation \(x y'' + (1-x) \, y' + n \, y = 0\).
\end{problem}

\begin{solution}
  Let $f = x^n e^{-x}$.  Since $f' = (n-x) x^{n-1} e^{-x}$, we note
  \[
    x f' + (x-n) f  = 0.
  \]
  Differentiating this relation $n+1$ times proves
  \begin{align*}
    0 &= \frac{d^{n+1}}{dx^{n+1}} (x f') + \frac{d^{n+1}}{dx^{n+1}} ((x-n) f) \\
      &= x f^{(n+2)} + (n+1) f^{(n+1)} + ( x - n ) f^{(n+1)} + (n+1) f^{(n)}
      &= x f^{(n+2)} + (1+x) f^{(n+1)} + (n+1) f^{(n)}
  \end{align*}
  Since $e^{-x} L_n(x) = f^{(n)}(x)$, we have
  \begin{align*}
    0 &= x \left( e^{-x} L_n(x) - e^{-x} L'_n(x) - e^{-x} L'_n(x) + e^{-x} L''_n(x)  \right) + (1+x) \left( - e^{-x} L_n(x) + e^{-x} L'_n(x) \right)  + (n+1) e^{-x} L_n(x)
  \end{align*}
  or equivalently
  \begin{align*}
    0 &= x  L''_n(x)  + (1-x) L'_n(x)  + n \, L_n(x)
  \end{align*}
  which is what we wanted to show.
\end{solution}

\begin{problem}
  Define an inner product via
  \[
    \langle f, g \rangle = \int_0^\infty e^{-x} f(x) \, g(x) \, dx.
  \]
  Then $\langle L_n, L_m \rangle = 0$ if $n \neq m$.
  
  \textit{Hint:}  Write the Laguerre equation in \textbf{Sturm-Liouville form,}
  \[
    \frac{d}{dx}\left(\left(x e^{-x}\right){\frac {d L_{n}(x)}{dx}}\right)+ n e^{-x} L_{n}(x)=0,
  \]
  which should evoke \ref{legendre-polynomials}.  Can you verify that $\langle D L_n, L_m \rangle = \langle L_n, DL_m \rangle$ where $Dy = (e^{-x} x y')'$, perhaps as an homage to \ref{legendre-orthogonal}.
\end{problem}

% BADBAD more problems involving gamma function -- maybe fractional derivatives?

\end{document}
